#ì±—ë´‡
import json
import os
import streamlit as st #ì›¹ ì¸í„°í˜ì´ìŠ¤
from streamlit_chat import message
from numpy import complex256
from sentence_transformers import SentenceTransformer  #íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸
from sklearn.metrics.pairwise import cosine_similarity  #ì½”ì‚¬ì¸ ìœ ì‚¬ë„
import pandas as pd
#stt/tts
import speech_recognition as sr
from speech_recognition import *
from gtts import gTTS
import time
import clipboard
import keyboard
import pyautogui
from pyautogui import *
import playsound
import pyaudio

#í˜ì´ì§€ ê¸°ë³¸ ì„¸íŒ…
st.set_page_config(page_title="í˜¸í‹° HOTI", page_icon="ğŸ¤–",) #í˜ì´ì§€ ì•„ì´ì½˜ ë° ì´ë¦„
st.image("https://ifh.cc/g/xNnht7.png", width=300,) #í•™êµ ë¡œê³ 
st.title('í˜¸í‹° HOTI') #ì œëª©
st.markdown("í˜¸ì¹˜ë¯¼ì‹œí•œêµ­êµ­ì œí•™êµ í™ˆí˜ì´ì§€ ì¸ê³µì§€ëŠ¥ ì±—ë´‡") #ì„¤ëª…

#TTS
def speak(text):
    tts = gTTS(text=text,lang='ko')
    filename = 'voice.mp3' #ë³€í™˜ëœ ìŒì„±ì„ mp3 íŒŒì¼ì— ì €ì¥
    tts.save(filename)
    playsound.playsound(filename) #ì¬ìƒ

#STT
def read_voice(): #ìŒì„± ì¸ì‹
    r = Recognizer() #ì¸ì‹ê¸°
    mic = Microphone() #ë§ˆì´í¬ ê°ì²´ ë¶ˆëŸ¬ì˜¤ê¸°
    with mic as source:
        audio = r.listen(source) #ìŒì„± ì½ì–´ì˜¤ê¸°
    voice_data = r.recognize_google(audio, language='ko') #êµ¬ê¸€ì˜ í•œêµ­ì–´ stt ëª¨ë¸ ì‚¬ìš©
    return voice_data
    
def typing(value): #í‚¤ë³´ë“œ ì…ë ¥
    clipboard.copy(value) #ë³µì‚¬
    pyautogui.hotkey('command', 'v') #ë¶™ì—¬ë„£ê¸° (pyautoguiì—ì„œëŠ” ì˜ì–´ë§Œ ë°”ë¡œ write í•  ìˆ˜ ìˆìŒ)

#Streamlit ì„¸íŒ…
@st.cache(allow_output_mutation=True)
def cached_model(): #ëª¨ë¸
    model = SentenceTransformer('jhgan/ko-sroberta-multitask') #Sentence Transformer ëª¨ë¸
    return model

@st.cache(allow_output_mutation=True)
def get_dataset(): #ë°ì´í„°ì…‹
    df = pd.read_csv('chat_real.csv') #ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹œ ë°ì´í„°ì…‹
    df['embedding'] = df['embedding'].apply(json.loads) 
    return df

model = cached_model()
df = get_dataset()

#Session state
if 'past' not in st.session_state:
    st.session_state['past'] = [] #ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±

if 'generated' not in st.session_state:
    st.session_state['generated'] = [] #ë‹µë³€ ë¦¬ìŠ¤íŠ¸ ìƒì„±

#ì…ë ¥ì°½ ìƒì„±
with st.form('form', clear_on_submit=True):
    user_input = st.text_input("ì´ê³³ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    col1, col2 = st.columns([9.6,1]) #ë²„íŠ¼ ìœ„ì¹˜ ì¡°ì •
    with col1:
        pass
    with col2:  
        submitted = st.form_submit_button('ì „ì†¡') #ì „ì†¡ ë²„íŠ¼

#ìŒì„±ì¸ì‹ ë²„íŠ¼ ìƒì„±
col1, col2 = st.columns([9.6, 1]) #ë²„íŠ¼ ìœ„ì¹˜ ì¡°ì •
with col1:
    pass
with col2:
    btn_clicked = st.button("ğŸ™", key="voice_btn") #ìŒì„±ì¸ì‹ ì‹œì‘ ë²„íŠ¼

if btn_clicked:
    voice = read_voice() 
    time.sleep(1)
    #print(voice)
    typing(voice) #ì…ë ¥ì°½ì— í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸°
    #time.sleep(0.5)
    #submitted = True #ìŒì„± ì¸ì‹ë˜ë©´ ë°”ë¡œ ì „ì†¡ ë²„íŠ¼ ëˆŒë ¤ì§

#ë‹µë³€ ìƒì„± ë‹¨ê³„
if submitted and user_input: #ì œì¶œì„ ëˆ„ë¥´ê³  í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ë©´
    embedding = model.encode(user_input) #ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©
    df['distance'] = df['embedding'].map(lambda x: cosine_similarity([embedding], [x]).squeeze()) #ìœ ì‚¬ë„ íŒì •
    answer = df.loc[df['distance'].idxmax()] #ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ê²ƒì„ ë‹µë³€ìœ¼ë¡œ ì„¤ì •
    
    st.session_state.past.clear() #ì§ˆë¬¸ ë‚´ì—­ ì´ˆê¸°í™”
    st.session_state.generated.clear() #í˜„ì¬ ë‹µë³€ ë‚´ì—­ ì´ˆê¸°í™”
    st.session_state.past.insert(0,user_input) #ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ 
    st.session_state.generated.insert(0,answer['ì±—ë´‡']) #ë‹µë³€ í‘œì‹œ

    message(st.session_state['past'][0], is_user=True, key=str(0) + '_user') #ì§ˆë¬¸ ì „ì†¡
    time.sleep(0.3) #ë”œë ˆì´
    message(st.session_state['generated'][0], key=str(0) + '_bot') #ë‹µë³€ ì „ì†¡
    k = st.session_state['generated'][0].index('h') #url ì‹œì‘ ì¸ë±ìŠ¤ ì°¾ê¸°
    speak(st.session_state['generated'][0][0:k]) #url ì œì™¸í•˜ê³  ë§í•˜ê¸°

    submitted = False #ë²„íŠ¼ ìƒíƒœ ë¦¬ì…‹
